:imagesdir: ../../assets/images
include::../style.adoc[]
:check: ✓
:cross: –


== Kafka Producer API

The Kafka producer allows developers to write kafka producers within their applications to send records to Kafka topics. The API provides various configuration options to control the behavior of the producer, such as batching, compression, retries, and acknowledgments.

We'll be looking at a simple Java application that uses the Kafka Producer API to send messages to a Kafka topic. The Java client libraries for Kafka streamline a lot of the work that goes into producing messages to and consuming messages from Kafka brokers.

=== The Simple Producer Application

. Open the Devspaces IDE from the top right corner of your OpenShift console.
+
image::m2/devspaces.png[]

. From the Devspaces dashboard, click on *open* button to launch your workspace.
+
image::m2/devspaces-launch.png[]

. Click on *Yes, I trust the authors* to proceed.
+
image::m2/devspaces-trust.png[]

. Click on the *pom.xml* file in the file explorer on the left side to open it.
+
image::m2/pom-xml.png[]

. Look at the dependencies section of the pom.xml file (line 15-21). You will see that the project has a dependency on the Kafka client library. This library provides the necessary classes and methods to interact with a Kafka cluster.
+
image::m2/dependency-pom.png[] 

. Let's us now look at the producer code. Click on the *src/main/java/com/example/KafkaProducerService.java* file in the file explorer to open it.
+
image::m2/kafka-producer-service.png[]

=== Understanding the Kafka Producer API (Java)


This section breaks down the code and explain hows a **Kafka Producer** is configured and how it works under the hood.


In this example:

* The producer connects to a Kafka cluster using configuration from `AppConfig`
* It sends messages with string keys and values
* It waits for acknowledgments to ensure reliability
* It safely closes when done

=== Producer Configuration

* The configuration is defined using `Properties` and constants from `ProducerConfig`. Look at lines 17-28 of the code.

This reference organizes producer configuration options by concern (batching, reliability, etc.).  
The **Used in example?** column shows whether the option appears in the sample code.

=== Basic Core Settings
_Configure how the producer connects and communicates with the Kafka cluster._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `bootstrap.servers`
| How the client finds the kafka cluster
| `HOST:PORT` (comma-separated)
| Seed brokers for metadata; must be reachable from client network.
| {check} line 20

| `client.id`
| Logical app identifier
| free string
| Useful for metrics/log tracing in multi-producer apps.
| {check} line 26
|===


=== Serialization
_Define how message keys and values are converted to bytes before transmission._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `key.serializer`
| How keys are converted to bytes
| `StringSerializer`, `ByteArraySerializer`, etc
| Must match the producer’s key type (and consumer deserializer).
| {check} line 21

| `value.serializer`
| How values are converted to bytes
| `StringSerializer`, `ByteArraySerializer`, Avro/JSON/Protobuf
| Option to implement custom serialization to handle complex types.
| {check} line 22
|===

=== Reliability & Delivery Guarantees
_Control how strictly the producer ensures message delivery and avoids duplicates._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `acks`
| Durability of writes
| `0`, `1`, `all`
| `all` - message written to leader and all replicas before ack. Strongest guarantee. `1` - leader ack only. `0` - no ack, fire-and-forget.
| {check} line 23

| `enable.idempotence`
| Duplicate prevention on retries
| `true` / `false`
| Producer will ensure that exactly one copy of each message is written in the stream Requires `acks=all`.
| {check} line 25
|===

[NOTE]
====
.Click to expand  and learn about other Reliability & Delivery Guarantees options
[%collapsible]
=====

[discrete]
=== Reliability & Delivery Guarantees
_Control how strictly the producer ensures message delivery and avoids duplicates._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `max.in.flight.requests.per.connection`
| Concurrent in-flight batches per connection
| `1`–`5` (with idempotence), higher without
| set this to `1` to ensure message order when retries occur.
| {cross} (implicitly constrained by idempotence)

| `retries`
| Automatic resend attempts
| integer (often large)
| With idempotence, safe to set high; bounded by `delivery.timeout.ms`.
| {cross}

| `delivery.timeout.ms`
| Overall send deadline (incl. retries)
| Default `120000` (2 min)
| If exceeded, the record fails even if `retries` remain.
| {cross}
|===
=====
====

=== Throughput & Batching
_Improve efficiency and throughput by controlling batching, compression, and buffering._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `linger.ms`
| Delaying the producer sending to give more time for batching
| `0`–`50+`
| Higher = better batching/throughput, higher latency. `0` = send ASAP.
| {check} line 24

|===

[NOTE]
====
.Click to expand  and learn about other Throughput & Batching options
[%collapsible]
=====

[discrete]
=== Throughput & Batching
_Improve efficiency and throughput by controlling batching, compression, and buffering._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `batch.size`
| Max bytes per batch
| e.g., `16384` (16 KiB), `32768`, `65536`
| Larger batches improve compression and throughput at cost of memory.
| {cross}

| `compression.type`
| Message compression
| `none`, `gzip`, `snappy`, `lz4`, `zstd`
| Compression covers full message batches. 
| {cross}

| `buffer.memory`
| Total bytes of memory the producer can use to buffer records waiting to be sent to the server.
| Default `33554432` (32 MiB)
| If records are sent faster than they can be delivered to the server the producer will block for max.block.ms after which it will fail with an exception
| {cross}
|===
=====
====


=== Timeouts & Backoff
_Define how long the producer waits and how it behaves during temporary failures._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `request.timeout.ms`
| timeout to avoid queuing records indefinitely
| e.g., `30000`
| timeout expires --> message will be removed from the queue --> throw exception
| {cross}

| `retry.backoff.ms`
| Delay between retries
| e.g., `100`–`1000`
| Avoids tight retry loops under transient errors.
| {cross}

| `metadata.max.age.ms`
| Metadata refresh interval
| e.g., `300000`
| Controls how often producer refreshes kafka cluster metadata.
| {cross}
|===

=== Producer Creation (line 27)
Then the producer is created:

[source,java]
----
this.producer = new KafkaProducer<>(props);
----

This initializes the producer with the specified configuration properties.

=== Sending a Message (line 34)

The `send()` method handles publishing to Kafka:

[source,java]
----
ProducerRecord<String, String> rec =
    (key == null || key.isEmpty())
        ? new ProducerRecord<>(config.topic, value)
        : new ProducerRecord<>(config.topic, key, value);

return producer.send(rec);
----

This builds a `ProducerRecord` (topic, optional key, value) and sends it to the Kafka cluster.


=== Closing the Producer (line 38-39)

Finally, the `close()` method ensures resources are released:

[source,java]
----
@Override
public void close() {
    try { producer.close(); } catch (Exception ignored) {}
}
----

Closing ensures:

* All buffered records are flushed.
* Pending sends are completed.
* Network resources are released.

== How This Producer Works (Recap)

. App builds a `ProducerRecord` (topic, optional key, value).
. `StringSerializer` converts key/value to bytes.
. Key (if present) determines partition (default hash).
. Records accumulate; `linger.ms=0` → send immediately (minimal batching).
. Sender thread transmits; `acks=all` ensures strong delivery.
. `enable.idempotence=true` eliminates duplicates on retry (per-partition).
. `close()` flushes and cleans up.

=== Summary

The producer lifecycle:

**Configure → Serialize → Partition → Batch → Send → Acknowledge → Close**

=== What's Next?
In the next section, you will learn about the Kafka Consumer API, which allows applications to read (consume) records from Kafka topics.