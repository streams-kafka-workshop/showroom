:imagesdir: ../../assets/images
include::../style.adoc[]
:check: ✓
:cross: ×


== Kafka Producer API

The Kafka producer allows developers to write kafka producers within their applications to send records to Kafka topics. The API provides various configuration options to control the behavior of the producer, such as batching, compression, retries, and acknowledgments.

We'll be looking at a simple Java application that uses the Kafka Producer API to send messages to a Kafka topic. The Java client libraries for Kafka streamline a lot of the work that goes into producing messages to and consuming messages from Kafka brokers.

=== A Simple Producer Application

* Navigate to {openshift_cluster_console}/k8s/cluster/projects[OpenShift Cluster Console, window="_console"]. Login with your username and password (`{user_name}`/`{password}`).

* Open the Devspaces IDE from the top right corner of your OpenShift console.
+
image::m2/devspaces.png[]

* Click on the login with OpenShift button.
+
image::m2/login-openshift.png[]

* Login using your credentials  - `{user_name}` and `{password}`

* Click on Allow Selected Permissions button to proceed.

* From the Devspaces dashboard, click on *open* button to launch your workspace.
+
image::m2/devspaces-launch.png[]

* Once the workspace is launched, Click on *Yes, I trust the authors* to proceed.
+
image::m2/devspaces-trust.png[]

* Expand the `module-kafka-api` folder and click on the `pom.xml` file to open it.
+
image::m2/module-kafka-api-pom-xml.png[]

* Look at the dependencies section of the pom.xml file (line 23-29). You will see that the project has a dependency on the Kafka client library. This library provides the necessary classes and methods to interact with a Kafka cluster.
+
image::m2/module-kafka-api-dependencies.png[]


=== Understanding the Kafka Producer API (Java)

This section breaks down the code and explain hows a **Kafka Producer** is configured and how it works under the hood.

* Let us now look at the producer code. In the `module-kafka-api` folder, click on the *src/main/java/com/example/SimpleKafkaProducer.java* file in the file explorer to open it.
+
image::m2/simple-kafka-producer-nav.png[]

In this example:

.  The producer connects to a Kafka cluster using configuration values from `KafkaConfig.java`. 
.  It accepts some of the configuration parameters as user inputs and changes the message sending behavior accordingly.
.  It sends messages with string keys and values
.  It safely closes when done

=== Producer Configuration
* The configuration is defined using `Properties` and constants from `ProducerConfig`. Look at lines 43-67 of the `SimpleKafkaProducer.java` file.

This reference organizes producer configuration options by concern (batching, reliability, etc.).  
The **Used in example?** column shows whether the option appears in the sample code.

=== Basic Core Settings
_Configure how the producer connects and communicates with the Kafka cluster._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `bootstrap.servers`
| How the client finds the kafka cluster
| `HOST:PORT` (comma-separated)
| Seed brokers for metadata; must be reachable from client network.
| {check} line 44

| `client.id`
| Logical app identifier
| free string
| Useful for metrics/log tracing in multi-producer apps.
| {cross}
|===


=== Serialization
_Define how message keys and values are converted to bytes before transmission._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `key.serializer`
| How keys are converted to bytes
| `StringSerializer`, `ByteArraySerializer`, etc
| Must match the producer’s key type (and consumer deserializer).
| {check} line 48

| `value.serializer`
| How values are converted to bytes
| `StringSerializer`, `ByteArraySerializer`, Avro/JSON/Protobuf
| Option to implement custom serialization to handle complex types.
| {check} line 49
|===

=== Reliability & Delivery Guarantees
_Control how strictly the producer ensures message delivery and avoids duplicates._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `acks`
| Durability of writes
| `0`, `1`, `all`
| `all` - message written to leader and all replicas before ack. Strongest guarantee. `1` - leader ack only. `0` - no ack, fire-and-forget.
| {check} line 55

| `enable.idempotence`
| Duplicate prevention on retries
| `true` / `false`
| Producer will ensure that exactly one copy of each message is written in the stream Requires `acks=all`.
| {cross}
|===

[NOTE]
====
.Click to expand  and learn about other Reliability & Delivery Guarantees options
[%collapsible]
=====
|===
| Option | What it controls | Typical values | Notes | Used in example?

| `max.in.flight.requests.per.connection`
| Concurrent in-flight batches per connection
| `1`–`5` (with idempotence), higher without
| set this to `1` to ensure message order when retries occur.
| {cross} (implicitly constrained by idempotence)

| `retries`
| Automatic resend attempts
| integer (often large)
| With idempotence, safe to set high; bounded by `delivery.timeout.ms`.
| {cross}

| `delivery.timeout.ms`
| Overall send deadline (incl. retries)
| Default `120000` (2 min)
| If exceeded, the record fails even if `retries` remain.
| {cross}
|===
=====
====

=== Throughput & Batching
_Improve efficiency and throughput by controlling batching, compression, and buffering._

|===
| Option | What it controls | Typical values | Notes | Used in example?

| `linger.ms`
| Delaying the producer sending to give more time for batching
| `0`–`50+`
| Higher = better batching/throughput, higher latency. `0` = send ASAP.
| {check} line 67

| `batch.size`
| Max bytes per batch
| e.g., `16384` (16 KiB), `32768`, `65536`
| Larger batches improve compression and throughput at cost of memory.
| {check} line 61

|===

[NOTE]
====
.Click to expand  and learn about other Throughput & Batching options
[%collapsible]
=====
|===
| Option | What it controls | Typical values | Notes | Used in example?

| `compression.type`
| Message compression
| `none`, `gzip`, `snappy`, `lz4`, `zstd`
| Compression covers full message batches. 
| {cross}

| `buffer.memory`
| Total bytes of memory the producer can use to buffer records waiting to be sent to the server.
| Default `33554432` (32 MiB)
| If records are sent faster than they can be delivered to the server the producer will block for max.block.ms after which it will fail with an exception
| {cross}
|===
=====
====

=== Timeouts & Backoff
_Define how long the producer waits and how it behaves during temporary failures._

[NOTE]
====
.Click to expand  and learn about Timeouts & Backoff options
[%collapsible]
=====
|===
| Option | What it controls | Typical values | Notes | Used in example?

| `request.timeout.ms`
| timeout to avoid queuing records indefinitely
| e.g., `30000`
| timeout expires --> message will be removed from the queue --> throw exception
| {cross}

| `retry.backoff.ms`
| Delay between retries
| e.g., `100`–`1000`
| Avoids tight retry loops under transient errors.
| {cross}

| `metadata.max.age.ms`
| Metadata refresh interval
| e.g., `300000`
| Controls how often producer refreshes kafka cluster metadata.
| {cross}
|===
=====
====

=== Producer Creation (line 76)
After all the options are defined, Then the producer is created using those properties.:

[source,java]
----
this.producer = new KafkaProducer<>(props);
----


=== Building a Message/Record (line 103)
The producer record is created with the topic name, key, and value.
[source,java]
----
ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
----

=== Sending a Message (line 1)

The `send()` uses the record created in the previous step and handles publishing to Kafka cluster:
[source,java]
----
producer.send(record, (metadata, exception)
----

=== Closing the Producer (line 38-39)

Finally, the `close()` method ensures resources are released:

[source,java]
----
public void close() {
        if (producer != null) {
            producer.close();
            System.out.println("Producer closed successfully.");
        }
    }
}
----

Closing ensures:

* All buffered records are flushed.
* Pending sends are completed.
* Network resources are released.


== How This Producer Works (Recap)

. App builds a `ProducerRecord` (topic, optional key, value).
. `StringSerializer` converts key/value to bytes.
. Key (if present) determines partition.
. Records accumulate; `linger.ms=0` → send immediately (minimal batching).
. Sender thread transmits; `acks=all` ensures strong delivery.
. `close()` flushes and cleans up.

=== Summary

The producer lifecycle:

**Configure → Serialize → Partition → Batch → Send → Acknowledge → Close**

=== Up Next
In the next section, we will see this producer in action and tweak some of the options to see how they affect the producer's behavior.



