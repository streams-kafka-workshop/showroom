:imagesdir: ../../assets/images
include::../style.adoc[]



== Event Streaming
This Content is work in Progress and is not ready for use. It will be soon.


Event streaming is a modern approach to building data-intensive applications that are responsive, resilient, and scalable. It enables real-time data processing and integration across diverse systems, making it ideal for today's fast-paced digital environments.
Event streaming treats every change in your business (an order placed, payment authorized, sensor reading, log line, click) as an event written to an append-only log. Instead of pushing data directly from one service to another, systems publish events once and many consumers independently read, process, and react—now or later.

image::intro/event-streaming.png[]

Key properties:

* **Durable log with retention:** Events stick around for hours, days, or longer, enabling replay and backfills.

* **Decoupling:** Producers and consumers evolve independently; new consumers can be added without changing producers.

* **Scalable fan-out:** One event can feed real-time apps, analytics, ML features, and audit trails at the same time.

=== Why Kafka?

Kafka is the most widely used event-streaming platform because it combines:

. **High throughput & low latency:** Horizontal scaling via partitions across brokers.

. **Per-partition ordering:** Reliable ordering for related keys (e.g., all events for customer X).

. **Replayability:** Consumers track their own offsets, so they can pause, catch up, or reprocess historic data.

. **Strong delivery options:** From at-least-once to exactly-once (with idempotence/transactions) for robust pipelines.

. **Ecosystem & interoperability:** Connectors, stream processing libraries, and standard APIs used across the industry.

=== Common Kafka use cases (with patterns you’ll recognize)

* **Payments & risk:** Real-time scoring on auth events; asynchronous settlement; chargeback analytics.

* **E-commerce:** Orders, inventory, pricing, and clickstream flowing to services and analytics in parallel.

* **IoT/telemetry:** Millions of device readings ingested and aggregated continuously.

* **Change Data Capture (CDC):** Database changes emitted as events to sync systems or build audit trails.

* **Operational analytics:** Streams to warehouses/lakes for near-real-time dashboards and anomaly detection.

* **Microservices choreography:** Services publish domain events; others react (SAGA patterns, outbox, DLT).

* **Log aggregation:** Unified transport for logs and security events with long-enough retention to reprocess.

=== When Kafka might not be the right fit
While we recommend Kafka for many use cases, there are some cases where it may not be the right fit.

* Simple RPC or synchronous request/response between two services (use HTTP/gRPC).

* Tiny workloads or one-off tasks where a lightweight queue is simpler.

* Huge binary blobs (store in object storage; send references via Kafka).

=== What is {streams}?

{streams} is Kafka built for OpenShift/Kubernetes, using the Strimzi operators to make Kafka declarative and automated:

* **CRDs & operators:** Define clusters, topics, and users in YAML; operators handle deployment, scaling, and upgrades. This workshop leverages the {streams} operators to install and manage Kafka clusters.

* **Day-2 operations:** Rolling updates, self-healing, storage management, and policy alignment with OpenShift.

* **Security & observability:** TLS, SASL/OAuth/mTLS, RBAC integration, Prometheus/Grafana metrics.

* **Ecosystem components:** Brokers (modern KRaft), Kafka Connect, MirrorMaker 2, and bridges—ready for platform teams.

* **Why it matters:** You get Kafka’s standard APIs with Kubernetes-native reliability, GitOps workflows, and platform governance—ideal for multi-team, multi-tenant environments.

=== Workshop Objectives

This workshop introduces you to the fundamentals of Event Streaming with {streams}. You will learn key concepts such as topics, partitions, producers, consumers, consumer groups, and replication. You will also gain hands-on experience by using a sample application to understand these concepts.

This workshop is intended for developers and architects who are new to event streaming and want to learn how to use Kafka for building real-time data pipelines and streaming applications. 




